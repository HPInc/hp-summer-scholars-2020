{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bring in Covid-19 Data for Business Insights\n",
    "\n",
    "Now that we have some basic enterprise data ready, let's bring in some Covid-19 data.  We'll merge these two data sets together to try to answer a few questions related to our business:\n",
    "\n",
    "- **Are there any regions where the Covid-19 case rate is rising week to week?**\n",
    "- **Are any of those in my top 100 sales regions?**\n",
    "- **Who are the affected salespeople so I can notify them?**\n",
    "\n",
    "To get started, we'll load in case rate data from the New York Times into a Postgres table.  There's a public data set here:\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/starschema.covid/NYT_US_COVID19.csv (<a href=\"https://github.com/nytimes/covid-19-data/blob/master/LICENSE\">license</a>)\n",
    "\n",
    "The NYT Covid-19 data also uses FIPS to designate the location of cases and deaths, so we can cross-reference that data with our sales data which is already indicated by FIPS.\n",
    "\n",
    "\n",
    "## 5.1 Create the table for the NYT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_connect import my_connect\n",
    "\n",
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "q = \"DROP TABLE IF EXISTS nyt_us_covid19;\"\n",
    "cursor.execute(q)\n",
    "connection.commit()\n",
    "\n",
    "q = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS nyt_us_covid19 (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                date DATE,\n",
    "                county VARCHAR(200),\n",
    "                state VARCHAR(100),\n",
    "                fips VARCHAR(5),\n",
    "                cases INTEGER,\n",
    "                deaths INTEGER,\n",
    "                iso3166_1 VARCHAR(10),\n",
    "                iso3166_2 VARCHAR(10),\n",
    "                cases_since_prev_day INTEGER,\n",
    "                deaths_since_prev_day INTEGER,\n",
    "                last_update_date TIMESTAMP,\n",
    "                last_reported_flag BOOLEAN\n",
    "               )\n",
    "\"\"\"\n",
    "cursor.execute(q)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Download the NYT Covid-19 data file from S3\n",
    "\n",
    "Download the CSV file from S3 and save a copy locally, referenced by CSV_FILE below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "SOURCE = \"https://s3-us-west-1.amazonaws.com/starschema.covid/NYT_US_COVID19.csv\"\n",
    "CSV_TEMP = os.path.join(os.getcwd(), \"nyt_latest.csv\")\n",
    "r = requests.get(SOURCE)\n",
    "open(CSV_TEMP, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Import the CSV into the nyt_us_covid19 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.sql as sql\n",
    "import pandas\n",
    "import os\n",
    "\n",
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Zero out the table first\n",
    "q = \"DELETE FROM nyt_us_covid19;\"\n",
    "cursor.execute(q)\n",
    "connection.commit()\n",
    "\n",
    "CSV_TEMP = os.path.join(os.getcwd(), \"nyt_latest.csv\")\n",
    "\n",
    "q2 = sql.SQL(\"\"\"\n",
    "COPY nyt_us_covid19(date, county, state, fips, cases, deaths, iso3166_1, iso3166_2,cases_since_prev_day, \n",
    "deaths_since_prev_day, last_update_date, last_reported_flag) FROM {} CSV HEADER;\n",
    "\"\"\")\n",
    "\n",
    "cursor.execute(q2.format(sql.Literal(CSV_TEMP)))\n",
    "connection.commit()\n",
    "\n",
    "os.unlink(CSV_TEMP)\n",
    "    \n",
    "df = pandas.io.sql.read_sql_query(\"SELECT * FROM nyt_us_covid19 ORDER BY date DESC LIMIT 5\", connection)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a table with all of the Covid-19 case data.  You can re-run this at any time to load the latest data.  Now let's do some preparation on our data to get it into a form we can use to apply it to our business questions.\n",
    "\n",
    "Recall the first question we want to answer:\n",
    "- **Are there any regions where the Covid-19 case rate is rising week to week?**\n",
    "\n",
    "To get this, we can run a query on the data, giving us last week's and this week's case rate by FIPS code, along with the difference.  In the second query below, note the following clause:\n",
    "```\n",
    "INTO cases_change_by_fips\n",
    "```\n",
    "\n",
    "This creates a new table with the results.  This intermediate table will make it easier to join up with our enterprise sales data later on.\n",
    "\n",
    "In these examples, we'll use the weeks of 5-April-2020 and 12-April-2020 as our 'current' weeks, even though by the time you read this they will no longer be current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# This selects the cases for the week of April 5 in each FIPS region into a temp table\n",
    "q1 = \"\"\"\n",
    "DROP TABLE IF EXISTS temp_table1;\n",
    "SELECT fips, \n",
    "SUM(CASE WHEN date BETWEEN '2020-04-05' AND '2020-04-11' AND fips <> 'None' THEN cases_since_prev_day ELSE 0.00 END) AS week1\n",
    "INTO TEMP TABLE temp_table1\n",
    "FROM nyt_us_covid19\n",
    "GROUP BY (fips);\n",
    "\"\"\"\n",
    "\n",
    "# This selects the cases for the subsequent week (April 12) and computes the difference\n",
    "q2 = \"\"\"\n",
    "DROP TABLE IF EXISTS cases_change_by_fips;\n",
    "SELECT nyt_us_covid19.fips, week1,\n",
    "SUM(CASE WHEN date BETWEEN '2020-04-12' AND '2020-04-18' AND nyt_us_covid19.fips <> 'None' \n",
    "    THEN cases_since_prev_day ELSE 0.00 END) - week1 AS case_change,\n",
    "-- This prevents division by zero when there are no new cases\n",
    "(CASE WHEN week1 = 0 THEN 0 ELSE \n",
    "  (SUM(CASE WHEN date BETWEEN '2020-04-12' AND '2020-04-18' AND nyt_us_covid19.fips <> 'None' \n",
    "    THEN cases_since_prev_day ELSE 0.00 END) - week1) / week1\n",
    "END) AS percent_change\n",
    "INTO cases_change_by_fips\n",
    "FROM nyt_us_covid19 \n",
    "JOIN temp_table1 ON (temp_table1.fips = nyt_us_covid19.fips)\n",
    "GROUP BY (nyt_us_covid19.fips, week1)\n",
    "ORDER BY case_change desc;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(q1)\n",
    "connection.commit()\n",
    "cursor.execute(q2)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Check the results\n",
    "\n",
    "Let's look at the top FIPS regions where the percent change from week to week was greater than 20% and the case count was at least 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "connection = my_connect()\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT *\n",
    "FROM cases_change_by_fips \n",
    "WHERE percent_change > .20 AND week1 > 100\n",
    "ORDER BY case_change desc\n",
    "\"\"\"\n",
    "df = pandas.io.sql.read_sql_query(q, connection)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the output should look like:\n",
    "\n",
    "```        \n",
    "    fips   week1  case_change  percent_change\n",
    "0  25017  2404.0       1021.0        0.424709\n",
    "1  44007   787.0        949.0        1.205845\n",
    "2  25009  1496.0        522.0        0.348930\n",
    "3  46099   357.0        481.0        1.347339\n",
    "4  06065   766.0        405.0        0.528721\n",
    "```\n",
    "\n",
    "### Data quality concern: spot-check your query results\n",
    "\n",
    "Let's spot-check some of the data to see if the query looks right.  Since 25017 is at the top of the list, let's look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.io.sql.read_sql_query(\"SELECT date, cases_since_prev_day FROM nyt_us_covid19  \\\n",
    "WHERE date BETWEEN '2020-04-05' AND '2020-04-11' AND fips = '25017'\", connection)\n",
    "print(df)\n",
    "print(df.sum())\n",
    "df = pandas.io.sql.read_sql_query(\"SELECT date, cases_since_prev_day FROM nyt_us_covid19 \\\n",
    "WHERE date BETWEEN '2020-04-12' AND '2020-04-18' AND fips = '25017'\", connection)\n",
    "print(df)\n",
    "print(df.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the output should look like:\n",
    "\n",
    "```\n",
    "         date  cases_since_prev_day\n",
    "0  2020-04-05                   164\n",
    "1  2020-04-06                   318\n",
    "2  2020-04-07                   237\n",
    "3  2020-04-08                   358\n",
    "4  2020-04-09                   500\n",
    "5  2020-04-10                   402\n",
    "6  2020-04-11                   425\n",
    "cases_since_prev_day    2404\n",
    "dtype: int64\n",
    "         date  cases_since_prev_day\n",
    "0  2020-04-12                   788\n",
    "1  2020-04-13                   323\n",
    "2  2020-04-14                   271\n",
    "3  2020-04-15                   427\n",
    "4  2020-04-16                   525\n",
    "5  2020-04-17                   538\n",
    "6  2020-04-18                   553\n",
    "cases_since_prev_day    3425\n",
    "dtype: int64\n",
    "```\n",
    "Looks good!  We had 2404 cases the first week, and 3425 the second week, a difference of 1021 cases, or (1021/2404 = .4247), which is what the first row of our table above says it should be.\n",
    "\n",
    "## 5.5 This answers our first business question:\n",
    "\n",
    "- **Are there any regions where the Covid-19 case rate is rising week to week?**\n",
    "\n",
    "The answer is yes, and we know exactly which regions are increasing by how much.  Now let's look at our second question:\n",
    "\n",
    "- **Are any of those in my top 100 sales regions?**\n",
    "\n",
    "We have all the data we need, we just need to put it together.  We'll join the case increase data we just created with the sales data by FIPS code.  Since we only care about significant case rate increases, we'll limit our search only to those regions where the percent_change is greater than 20% (0.2).\n",
    "\n",
    "To make it easier to understand, we'll create some intermediate tables to hold results, then we'll join them together at the end.\n",
    "\n",
    "The first table we need is the sales data summarized by FIPS.  For that, we'll create a table called total_sales_by_fips.  Here's the schema and the query:\n",
    "\n",
    "## 5.6 Create a total_sales_by_fips table for the top 100 sales regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: on my machine, this SELECT...INTO query will sometimes hang in Postgres 10.  Restarting the service fixes it.\n",
    "\n",
    "from my_connect import my_connect\n",
    "import pandas\n",
    "\n",
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"DROP TABLE IF EXISTS total_sales_by_fips;\")\n",
    "connection.commit()\n",
    "\n",
    "# Select all sales data summarized by each FIPS code for the top 100 FIPS\n",
    "q = \"\"\"\n",
    "SELECT SUM(sales.amount) AS total_sales, fips \n",
    "INTO total_sales_by_fips\n",
    "FROM sales\n",
    "GROUP BY (fips)\n",
    "ORDER BY total_sales DESC LIMIT 100;\n",
    "\"\"\"\n",
    "cursor.execute(q)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have data\n",
    "df = pandas.io.sql.read_sql_query(\"SELECT * FROM total_sales_by_fips\", connection)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Join the case change rate data with the top-100 regions data.\n",
    "\n",
    "Now all we need to do is join the cases_change_by_fips data with the total_sales_by_fips data and see if there's any intersection.  We can use an INNER JOIN for this, which will only generate a result where the FIPS matches between both.  Since our total_sales_by_fips table only has the top 100 FIPS regions we care about, we'll get a list of those top regions that have a concerning case rate increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT total_sales, cases_change_by_fips.fips, week1 AS first_week_cases, case_change, percent_change \n",
    "FROM total_sales_by_fips\n",
    "INNER JOIN cases_change_by_fips ON total_sales_by_fips.fips = cases_change_by_fips.fips\n",
    "WHERE cases_change_by_fips.percent_change > .20 AND cases_change_by_fips.week1 > 100\n",
    "ORDER BY cases_change_by_fips.percent_change DESC;\n",
    "\"\"\"\n",
    "connection = my_connect()\n",
    "df = pandas.io.sql.read_sql_query(q, connection)\n",
    "print(df.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the output should look like:\n",
    "```\n",
    "    total_sales   fips  first_week_cases  case_change  percent_change\n",
    "0     211503.33  27053             212.0        199.0        0.938679\n",
    "1     166378.42  13089             309.0        251.0        0.812298\n",
    "2     208426.75  06081             114.0         72.0        0.631579\n",
    "3     195980.34  48439             322.0        202.0        0.627329\n",
    "4     201578.86  13067             252.0        147.0        0.583333\n",
    "5     170168.17  48453             260.0        142.0        0.546154\n",
    "6     190369.15  06065             766.0        405.0        0.528721\n",
    "7     219742.09  06029             164.0         86.0        0.524390\n",
    "8     216130.19  39049             337.0        172.0        0.510386\n",
    "9     214920.82  25017            2404.0       1021.0        0.424709\n",
    "10    197162.99  48141             173.0         63.0        0.364162\n",
    "11    191036.56  12103             104.0         37.0        0.355769\n",
    "12    190502.24  25009            1496.0        522.0        0.348930\n",
    "13    202892.51  13135             281.0         97.0        0.345196\n",
    "14    224533.30  47157             441.0        142.0        0.321995\n",
    "15    179627.83  08031             413.0        128.0        0.309927\n",
    "16    187676.02  17043             458.0        136.0        0.296943\n",
    "17    178294.47  13121             487.0        132.0        0.271047\n",
    "18    194325.48  25027             907.0        223.0        0.245865\n",
    "19    214895.50  09003            1106.0        258.0        0.233273\n",
    "20    181039.78  51059             559.0        128.0        0.228980\n",
    "21    171498.17  12057             201.0         44.0        0.218905\n",
    "```\n",
    "\n",
    "## 5.8 Business insight:\n",
    "\n",
    "Remember our second business-related question:\n",
    "\n",
    "- **Are any of those in my top 100 sales regions?**\n",
    "\n",
    "Out of the top 100 counties we sell product in, 21 of them have had a significant increase in the number of cases.  That is a concern, and we should notify the relevant salespeople so they can react accordingly, hence our third question:\n",
    "\n",
    "- **Who are the affected salespeople so I can notify them?**\n",
    "\n",
    "We'll join the salesperson data based on the sate in which the FIPS region resides.  We'll start with the query above and modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "DROP TABLE IF EXISTS final_report;\n",
    "SELECT total_sales, \n",
    "       cases_change_by_fips.fips, \n",
    "       fips.area_name, \n",
    "       fips.state, \n",
    "       percent_change, \n",
    "       salesperson.name\n",
    "INTO final_report\n",
    "FROM total_sales_by_fips\n",
    "INNER JOIN cases_change_by_fips ON total_sales_by_fips.fips = cases_change_by_fips.fips\n",
    "INNER JOIN fips ON fips.fipstxt = cases_change_by_fips.fips\n",
    "INNER JOIN salesperson ON salesperson.state = fips.state\n",
    "WHERE cases_change_by_fips.percent_change > .20 and cases_change_by_fips.week1 > 100\n",
    "ORDER BY name;\n",
    "\"\"\"\n",
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(q)\n",
    "connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "q2 = \"\"\"\n",
    "SELECT total_sales AS \"Total Sales\", \n",
    "       fips AS \"FIPS\", \n",
    "       area_name AS \"County\", \n",
    "       state AS \"State\", \n",
    "       percent_change AS \"Increase\", \n",
    "       name AS \"Salesperson Name\"\n",
    "FROM final_report\n",
    "\"\"\"\n",
    "connection = my_connect()\n",
    "df = pandas.io.sql.read_sql_query(q2, connection)\n",
    "\n",
    "# Pandas dataframe number formatting and inline bar chart example\n",
    "df2 = df.style.bar(subset=['Increase'], align='mid', color=['#FF8888'])\\\n",
    "        .format({'Total Sales': \"{:,.2f}\", 'Increase': \"{:.0%}\"})\n",
    "# Nicer display inside a Jupyter notebook\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...and there you have it.\n",
    "\n",
    "We now have a list of the affected salespeople, and we've given them enough information that they can take whatever action they think is needed -- changing inventory levels, contacting stores in the region, checking for any local lockdown orders that might affect sales, etc.\n",
    "\n",
    "### Sales Regions: Covid-19 Weekly Case Increases of > 20% \n",
    "<img src=\"images/final-report.png\" align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: mapping the case rate increases\n",
    "\n",
    "You can use a tool like PowerBI to geographically map the case rate change data.  See <a href=\"https://docs.microsoft.com/en-us/power-bi/visuals/desktop-shape-map\">this link</a> for more details on how to enable Shape Maps in PowerBI.  Since our case rate data contains FIPS locations, Shape Maps can map it directly.  Here's an example using the ```final_report``` table; I had to work with the settings and filtering for a while to get what I wanted, but this should give you a sense of what's possible.  \n",
    "\n",
    "<img src=\"images/map-percent-change-2.png\">\n",
    "\n",
    "Other tools such as Tableau also have this capability.\n",
    "\n",
    "# Next notebook: next steps, automating, etc.\n",
    "\n",
    "<a href=\"6. Next Steps.ipynb\">Go to the next notebook -&gt;</a>\n",
    "\n",
    "\n",
    "*Contents Â© Copyright 2020 HP Development Company, L.P. SPDX-License-Identifier: MIT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
