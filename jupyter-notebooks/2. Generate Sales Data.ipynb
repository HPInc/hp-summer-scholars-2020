{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"1. FIPS Code and Population Data.ipynb\">&lt;- Back to previous notebook</a>\n",
    "\n",
    "# Step 2: Sourcing sales data.\n",
    "\n",
    "In this section we'll generate some fake sales data.  Normally you would get this from some enterprise sales system, partners if you're using resellers, etc.  Let's say we want some data that has the amount of the sale, the date/time the purchase was made, the location (county/FIPS), and some unique transaction ID.\n",
    "\n",
    "<img src=\"images/sample-sales.png\">\n",
    "\n",
    "### Data quality concern: data in context\n",
    "\n",
    "Immediately, though, we have some questions:\n",
    "- Amount: is that in US dollars?  Local currency if sold outside the US?  If we have to convert it, what conversion rate do we use - today's, or the one at the time of purchase?  When rounding, do you round up or truncate?  Accuracy is critical, especially when there's money involved.\n",
    "- Date/time: is that the date/time of the purchase in local time?  Daylight Saving Time?  What timezone was the purchase in?\n",
    "\n",
    "### The Data Catalog\n",
    "\n",
    "In the examples above, it's really important for a Subject Matter Expert (SME) that's familiar with the sales data to clearly define each of these fields and what they represent.  This can be recorded in a data catalog entry for the data source.  For example, the catalog entry for this table might look like this:\n",
    "\n",
    "| Column | Type | Description |\n",
    "| :-- | :-- | :---- |\n",
    "| amount | decimal(8, 2) | Amount in USD, rounded to the nearest cent.  Conversion from non-USD is done at the time of transaction with the conversion rate at midnight UTC of the date of purchase. |\n",
    "| trans_time | <a href=\"https://www.postgresql.org/docs/current/datatype-datetime.html\">timestamp</a> | Date/time of purchase in UTC |\n",
    "| id | <a href=\"https://www.postgresql.org/docs/current/datatype-uuid.html\">uuid</a> | A GUID representing a globally unique identifier for the transaction. |\n",
    "| fips | varchar(5) | The FIPS code (county/state) where the purchase was made |\n",
    "\n",
    "A data catalog entry might also contain information about data stewards or subject matter experts, the lineage of the table (e.g. joins with other tables), sample data, and more.\n",
    "\n",
    "\n",
    "## 2.1 Generating the sales data\n",
    "\n",
    "We'll use <a href=\"https://faker.readthedocs.io/en/master/\">faker</a>, an excellent Python library, to generate a bunch of fake sales information.  To make the example more interesting, though, we'll want to make sure that we weight our fake \"purchases\" more into the top 100 sales regions (FIPS codes) we have, similar to what would likely happen in real life.  So, 50% of the time, we'll pick at random one of the top 100 FIPS codes we inserted in the last notebook.  The other 50% of the time we'll look up a random record in the fips table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Read FIPS codes into lists for easy/fast retrieval\n",
    "\n",
    "### Data quality concern: the FIPS table has state and county data intermingled\n",
    "\n",
    "We don't want the state-level data for this next section, so we'll filter out anything that ends in '000' except for Washington DC.\n",
    "\n",
    "Another way we could have handled this would have been to delete that data when we imported it, but it's better this way -- if anyone else in my organization ever wants to reuse this data set, the entire set of data is there for them to use.  We can make a note of the state and county info being in there in the data catalog entry for this data set, if we have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's read the top 100 FIPS codes into a list in memory.  Pandas makes this extremely easy:\n",
    "from my_connect import my_connect\n",
    "import pandas\n",
    "\n",
    "connection = my_connect()\n",
    "\n",
    "# In this case, we only want counties; the WHERE clause filters out state-level data appropriately\n",
    "q = \"\"\"\n",
    "SELECT fipstxt FROM fips \n",
    "WHERE NOT(fipstxt LIKE '%000' AND state <> 'DC')\n",
    "ORDER BY pop_estimate_2019 DESC LIMIT 100\n",
    "\"\"\"\n",
    "df = pandas.io.sql.read_sql_query(q, connection)\n",
    "top_fips = df['fipstxt'].values.tolist()\n",
    "\n",
    "# Now we'll get all valid FIPS codes.\n",
    "q = \"SELECT fipstxt FROM fips WHERE NOT(fipstxt LIKE '%000' AND state <> 'DC')\"\n",
    "df = pandas.io.sql.read_sql_query(q, connection)\n",
    "all_fips = df['fipstxt'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create the 'sales' table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "\n",
    "q = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales (\n",
    "    id UUID PRIMARY KEY,\n",
    "    trans_time TIMESTAMP,\n",
    "    amount DECIMAL(8, 2),\n",
    "    fips VARCHAR(5)\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(q)\n",
    "connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Quick helper function for inserting each sales row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sale(connection, id, trans_time, amount, fips):\n",
    "    cursor = connection.cursor()\n",
    "    q = sql.SQL(\"INSERT INTO sales (id, trans_time, amount, fips) VALUES ({}, {}, {}, {});\")\n",
    "    cursor.execute(q.format(sql.Literal(str(id)), sql.Literal(trans_time), sql.Literal(amount), sql.Literal(fips)))\n",
    "    connection.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Generate fake sales data and insert it into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "import uuid\n",
    "import psycopg2.sql as sql\n",
    "\n",
    "connection = my_connect()\n",
    "cursor = connection.cursor()\n",
    "random.seed()\n",
    "fake = Faker()\n",
    "\n",
    "# Zero out the table before starting\n",
    "cursor.execute(\"DELETE FROM sales;\")\n",
    "connection.commit()\n",
    "\n",
    "TOTAL_RECORDS = 50000\n",
    "\n",
    "for i in range(TOTAL_RECORDS):\n",
    "    id = uuid.uuid4()\n",
    "    trans_time = fake.date_time_between(start_date='-1y', end_date='-1d')\n",
    "    amount = fake.pyfloat(left_digits=4, right_digits=2, positive=True, min_value=10, max_value=1500)\n",
    "\n",
    "    # 50% chance of picking a FIPS from the top FIPS to help skew our fake sales into heavily populated regions\n",
    "    if (random.choice([\"Top\", \"Random\"]) is \"Top\"):\n",
    "        fips = random.choice(top_fips)\n",
    "    else:\n",
    "        fips = random.choice(all_fips)\n",
    "        \n",
    "    insert_sale(connection, id, trans_time, amount, fips)\n",
    "    \n",
    "    # Print a status message every 5000 rows\n",
    "    if (i % 5000) == 0:\n",
    "        print(\"%s records inserted\" % i)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Aggregation: show total sales for the top 10 states\n",
    "\n",
    "Now that we have some sales data in there, we can start to get a little value out of it.  Let's say we want to aggregate the data by state and see which states have the highest sales.  Here's an example query:\n",
    "\n",
    "```\n",
    "SELECT SUM(sales.amount) AS total, fips.state AS state FROM sales\n",
    "INNER JOIN fips ON sales.fips = fips.fipstxt\n",
    "GROUP BY (fips.state)\n",
    "ORDER BY total DESC LIMIT 10;\n",
    "```\n",
    "\n",
    "Here's an example of what the result will look like:\n",
    "```\n",
    "        total state\n",
    "0  3312283.72    TX\n",
    "1  3173485.84    CA\n",
    "2  2113674.22    NY\n",
    "3  2017619.26    FL\n",
    "4  1627246.43    GA\n",
    "5  1380399.77    IL\n",
    "6  1106723.17    OH\n",
    "7  1036475.05    MA\n",
    "8  1023290.36    MO\n",
    "9  1003630.76    MI\n",
    "```\n",
    "\n",
    "Here's the actual query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "connection = my_connect()\n",
    "q = \"\"\"\n",
    "SELECT SUM(sales.amount) AS total, fips.state AS state FROM sales\n",
    "INNER JOIN fips ON sales.fips = fips.fipstxt\n",
    "GROUP BY (fips.state)\n",
    "ORDER BY total DESC LIMIT 10;\n",
    "\"\"\"\n",
    "df = pandas.io.sql.read_sql_query(q, connection)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next notebook: adding salespeople\n",
    "\n",
    "Next we will add in salesperson information so we can see who the top salespeople are.\n",
    "\n",
    "<a href=\"3. Generate Salespeople.ipynb\">Go to the next notebook -&gt;</a>\n",
    "\n",
    "\n",
    "*Contents Â© Copyright 2020 HP Development Company, L.P. SPDX-License-Identifier: MIT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
